<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Re-LAMP 2025 Workshop - Call for papers</title>
    <title>Efficient Microarchitectures for Resilient Large Model Processing  Workshop</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            padding: 0;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        ul {
            margin: 10px 0;
            padding-left: 20px;
        }
        li {
            margin-bottom: 5px;
        }
        .important-dates, .topics, .submission-guidelines, .program-committee, .contact {
            margin-bottom: 20px;
        }
    </style>
</head>
<body>

     <figure>
        <img src="Workshop logo.png" alt=" " 
            style="width: 25%; height: auto;"/>
    </figure>
    
    <h1>Re-LAMP 2025 Workshop - Call for papers</h1>
    <h1>Efficient Microarchitectures for Resilient Large Model Processing Workshop</h1>
    <p><strong>Held in conjunction with <a href="https://microarch.org/micro58/">MICRO 2025</a></strong></p>
    <p><strong>October 18th, 1pm-5pm @ Seoul, South Korea</strong></p>
    <p>Room: TBD </p>



   
    <h3>About the Workshop</h3>
    <p>The rapid evolution of Large Language Models (LLMs) and the emergence of Large Multimodal Models (LMMs) are revolutionizing various domains. Simultaneously, the pursuit of very long-context LLMs (e.g., 1M context length) is pushing the boundaries of what these models can achieve. However, the immense computational, memory, and power requirements of these advanced models present formidable challenges to current hardware and system designs.</p>
    <p>Fortunately, large models, including LLMs, LMMs, and those handling extended contexts, often exhibit inherent resiliency to noise and approximation. This workshop aims to harness this property by exploring microarchitectural innovations and system-level techniques that exploit such resiliency to significantly improve performance, power efficiency, and memory utilization. Our focus will extend beyond traditional LLMs to encompass the unique challenges and opportunities presented by multimodal data and extremely long contexts.</p>
    <p>Topics will include, but are not limited to, approximate computing, dynamic quantization, and adaptive methods that apply different levels of approximation or quantization across layers within these complex models. Additionally, the workshop will address critical memory efficiency concerns through novel data compression techniques that leverage model resiliency to reduce memory footprint, especially crucial for LMMs and long-context LLMs, while maintaining or even improving model performance.</p>
    <p>By bringing together researchers, practitioners, and industry experts, the ReLAMP workshop seeks to foster discussions and drive advancements in efficient microarchitectures and systems for the next generation of large model processing. This will pave the way for more sustainable, scalable, and capable AI solutions.</p>

    <div class="important-dates">
        <h3>Important Dates</h3>
        <ul>
           <li><strong>Paper Submission Deadline:</strong> <s>31 August 2025</s> 18 September 2025</li>
            <li><strong>Notification of Acceptance:</strong> 1 October 2025</li>
            <li><strong>Camera-Ready Submission:</strong> 30 September 2025</li>
            <li><strong>Workshop Date:</strong> 18 October 2025 </li>
        </ul>
    </div>

    <div class="topics">
        <h3>Topics of Interest</h3>
        <p>We invite submissions on a wide range of topics related to efficient processing and memory optimization for Large Language Models (LLMs), Large Multimodal Models (LMMs), and very long-context LLMs, including but not limited to:</p>
        <ul>
            <li>Microarchitectures for power-efficient processing of LLMs, LMMs, and long-context LLMs.</li>
            <li>Approximate computing techniques for large models, including structured and unstructured sparsity.</li>
            <li>Dynamic quantization and mixed-precision approaches tailored for diverse layers in LLMs, LMMs, and models with extended contexts.</li>
            <li>Adaptive approximation methods leveraging the unique characteristics of LLM and LMM layers, and long-context attention mechanisms.</li>
            <li>Techniques for leveraging model resiliency to enhance memory efficiency in LLMs, LMMs, and long-context LLMs.</li>
            <li>Novel data compression methods for reducing the memory footprint of large models, especially considering multimodal data and massive context windows.</li>
            <li>Hardware-software co-design for optimized processing of LLMs, LMMs, and very long-context LLMs.</li>
            <li>Trade-offs between accuracy, power, performance, and context length in large model optimization.</li>
            <li>Case studies and benchmarks for efficient processing of LLMs, LMMs, and extreme long-context models.</li>
            <li>Emerging technologies and memory solutions for sustainable deployment of next-generation large models.</li>
        </ul>
    </div>

    <div class="submission-guidelines">
        <h3>Submission Guidelines</h3>
        <p>ReLAMP welcomes submissions of short papers, up to 3 pages excluding references, using a double-column format. You can use <a href="ReLAMP2025_paper_template-2.tex.zip">this Latex template</a>.</p>
        <ul>
            <li>Submissions should clearly state the research problem, motivation, and technical contribution. All submissions must be in English.</li>
            <li>Please submit your paper as a single PDF file.</li>
            <li>Papers can present work in progress, exploratory/preliminary research, or already published work.</li>
            <li>Submissions will be assessed based on their novelty, technical quality, potential impact, interest, clarity, relevance, and reproducibility.</li>
            <li>Reviews will not be blind, so please submit without anonymizing authors in the submitted PDF.</li>
            <li>There will be no formal proceedings, allowing authors the flexibility to extend and publish their work in other conferences and journals.</li>
            <li>For each accepted paper, at least one author must attend the workshop and present the paper.</li>
        </ul>
        
 <h3 id="paper-submission-system">Paper submission system:</h3>
    <p><a href="https://cmt3.research.microsoft.com/ReLAMP2025">Submit your paper here</a> <br />
    Please note you have to be a CMT registered user to submit. <br />      
    <a href="https://cmt3.research.microsoft.com/User/Register">Register to CMT here</a></p>

    </div>

    <div class="program-committee">
        <h3>Program Committee</h3>
        <ul>
            <li>Prof. Freddy Gabbay, The Hebrew University</li>
            <li>Prof. Kingsum Chow, Zhejiang University</li>
            <li>Prof. Tony Wu, Zhejiang University</li>
            <li>Prof. Leng Jinwei, Shanghai Jiao Tong University</li>
            <li>Dr. Zhou Yigang, Huawei</li>
            <li>Dr. Wang Junsong, Huawei</li>
            <li>Dr. Emily Rozenshine, Huawei Europe</li>
        </ul>
    </div>

    <div class="contact">
        <h3>Contact Us</h3>
        <p>Any questions may be directed to: <a href="mailto:freddy.gabbay@mail.huji.ac.il">freddy.gabbay@mail.huji.ac.il</a></p>
    </div>

    <p>The Microsoft CMT service was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.</p>

</body>
</html>
